<!DOCTYPE html>
<html lang="zh-cn">
  <head><meta name="generator" content="Hexo 3.8.0">
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">

<meta name="theme-color" content="#f8f5ec">
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">



  <meta name="description" content="1. Linear Regression">




  <meta name="keywords" content="机器学习, Rookie">










  <link rel="alternate" href="/atom.xml" title="Rookie">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.10.2">



<link rel="canonical" href="https://zhangydong.top/2019/01/04/Linear-Regression/">



  <link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css">




  <link rel="stylesheet" type="text/css" href="/lib/nprogress/nprogress.min.css">



<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.10.2">



  


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110425765-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110425765-2');
</script>


  <script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>





  <script src="//cdn1.lncld.net/static/js/3.1.1/av-min.js"></script>
  <script id="leancloud">
    AV.init({
      appId: "3wSIBDvbuwPeOwNdvk5zjLHf-gzGzoHsz",
      appKey: "6GVMLrROY8HRyVW4odP5iKWI"
    });
  </script>





<script>
  window.config = {"leancloud":{"app_id":"3wSIBDvbuwPeOwNdvk5zjLHf-gzGzoHsz","app_key":"6GVMLrROY8HRyVW4odP5iKWI"},"toc":true,"fancybox":true,"pjax":true};
</script>

    <title> 1. Linear Regression - Rookie </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="mobile-header-logo">
    <a href="/." class="logo">Rookie</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    
      <a href="/">
        <li class="mobile-menu-item">
          
          
            首页
          
        </li>
      </a>
    
      <a href="/archives/">
        <li class="mobile-menu-item">
          
          
            归档
          
        </li>
      </a>
    
      <a href="/tags">
        <li class="mobile-menu-item">
          
          
            标签
          
        </li>
      </a>
    
      <a href="/categories">
        <li class="mobile-menu-item">
          
          
            分类
          
        </li>
      </a>
    
      <a href="/about">
        <li class="mobile-menu-item">
          
          
            关于
          
        </li>
      </a>
    
  </ul>
</nav>

    <div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Rookie</a>
</div>

<nav class="site-navbar">
  
    <ul id="menu" class="menu">
      
        <li class="menu-item">
          <a class="menu-item-link" href="/">
            
            
              首页
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            
            
              归档
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/tags">
            
            
              标签
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/categories">
            
            
              分类
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/about">
            
            
              关于
            
          </a>
        </li>
      
    </ul>
  
</nav>

      </header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            
  
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          1. Linear Regression
        
      </h1>

      <div class="post-meta">        
        
        <span class="post-time">
          2019-01-04
        </span>
        
          <span class="post-category">
            
              <a href="/categories/机器学习/">机器学习</a>
            
          </span>
        
        
        <span class="post-visits" data-url="/2019/01/04/Linear-Regression/" data-title="1. Linear Regression">
          阅读次数 0
        </span>
        
      </div>
    </header>

    
    
  <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">文章目录</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#What-is-Machine-Learning"><span class="toc-text">What is Machine Learning?</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Supervised-learning"><span class="toc-text">1. Supervised learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Linear-Regression"><span class="toc-text">Linear Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Cost-Function"><span class="toc-text">1. Cost Function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Gradient-Descent"><span class="toc-text">2. Gradient Descent</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Normal-Equation"><span class="toc-text">3. Normal Equation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-Unerfitting-and-Overfitting"><span class="toc-text">4. Unerfitting and Overfitting</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Unsupervised-learning"><span class="toc-text">2. Unsupervised learning</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sklearn"><span class="toc-text">sklearn</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-sklearn-linear-model-LinearRegression"><span class="toc-text">1. sklearn.linear_model.LinearRegression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Linear-Regression-Example"><span class="toc-text">2. Linear Regression Example</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#参考："><span class="toc-text">参考：</span></a></li></ol>
    </div>
  </div>



    <div class="post-content">
      
        <!-- TOC -->    
<a id="more"></a>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>

<img src="/2019/01/04/Linear-Regression/1stDay.PNG">
<h1 id="What-is-Machine-Learning"><a href="#What-is-Machine-Learning" class="headerlink" title="What is Machine Learning?"></a>What is Machine Learning?</h1><p>Two definitions of Machine Learning are offered. Arthur Samuel described it as: “the field of study that gives computers the ability to learn without being explicitly programmed.” This is an older, informal definition.</p>
<p>Tom Mitchell provides a more modern definition: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”</p>
<p>Example: playing checkers.</p>
<p>E = the experience of playing many games of checkers</p>
<p>T = the task of playing checkers.</p>
<p>P = the probability that the program will win the next game.</p>
<p>In general, any machine learning problem can be assigned to one of two broad classifications:</p>
<p><strong>Supervised learning</strong> and <strong>Unsupervised learning</strong>.</p>
<h2 id="1-Supervised-learning"><a href="#1-Supervised-learning" class="headerlink" title="1. Supervised learning"></a>1. Supervised learning</h2><p>In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.</p>
<table>
<thead>
<tr>
<th style="text-align:left">terminology</th>
<th style="text-align:center">notations</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>Features</strong> (input variables)</td>
<td style="text-align:center">$x ^ { ( i ) }$</td>
</tr>
<tr>
<td style="text-align:left"><strong>Target</strong> (output variables)</td>
<td style="text-align:center">$y ^ { ( i ) }$</td>
</tr>
<tr>
<td style="text-align:left"><strong>Training Example</strong></td>
<td style="text-align:center">$\left( x ^ { ( i ) } , y ^ { ( i ) } \right)$</td>
</tr>
<tr>
<td style="text-align:left"><strong>Training Set</strong></td>
<td style="text-align:center">${(x^{(i)} , y^{(i)} ); i = 1, \dots, m}$</td>
</tr>
<tr>
<td style="text-align:left">the <strong>Space</strong> of input values</td>
<td style="text-align:center">$\mathcal { X }$</td>
</tr>
<tr>
<td style="text-align:left">the <strong>Space</strong> of output values</td>
<td style="text-align:center">$\mathcal { Y }$</td>
</tr>
<tr>
<td style="text-align:left"><strong>Hypothesis</strong></td>
<td style="text-align:center">function $h : \mathcal { X } \mapsto \mathcal { Y }$</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:center">target variables</th>
<th style="text-align:center">problems</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">discrete</td>
<td style="text-align:center">classification</td>
</tr>
<tr>
<td style="text-align:center">continuous</td>
<td style="text-align:center">regression</td>
</tr>
</tbody>
</table>
<h3 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h3><p>$$h _ { \theta } ( x ) = \theta _ { 0 } + \theta _ { 1 } x _ { 1 } + \theta _ { 2 } x _ { 2 }$$<br>where $\theta _ { i }$ are the <strong>parameters</strong> ( also called <strong>weights</strong> ) parameterizing the<br>space of linear functions mapping from $\mathcal { X }$ to $\mathcal { Y }$.</p>
<p>$$h ( x ) = \sum _ { i = 0 } ^ { n } \theta _ { i } x _ { i } = \theta ^ { T } x$$<br>here $x _ { 0 } = 1$  <em>convention</em> ( this is the <strong>intercept term</strong> ), $\theta$ and $x$ are vectors, and $n$ is the number of input variables ( not counting $x_0$ ).</p>
<h4 id="1-Cost-Function"><a href="#1-Cost-Function" class="headerlink" title="1. Cost Function"></a>1. Cost Function</h4><p>Q: Given a training set, how do we pick, or learn, the parameters $\theta$ ?<br>A: To make $h ( x )$ close to $y$, at least for the training examples we have.</p>
<p>$$J ( \theta ) = \frac { 1 } { 2 } \sum _ { i = 1 } ^ { m } \left( h _ { \theta } \left( x ^ { ( i ) } \right) - y ^ { ( i ) } \right) ^ { 2 }$$</p>
<p>Q: How to minimize $J ( \theta )$ ?<br>A: <strong>gradient descent</strong><sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>, <strong>normal equation</strong> </p>
<p><a href="https://www.npmjs.com/package/hexo-footnotes" target="_blank" rel="noopener">脚注</a> ?</p>
<h4 id="2-Gradient-Descent"><a href="#2-Gradient-Descent" class="headerlink" title="2. Gradient Descent"></a>2. Gradient Descent</h4><p>$$\theta _ { j } : = \theta _ { j } - \alpha \frac { \partial } { \partial \theta _ { j } } J ( \theta )$$<br>Here, $\alpha$ is called <strong>learning rate</strong>.</p>
<p>2.1. for a single training example:<br><strong>LMS</strong> (least mean squares) update rule:<br>$$\theta _ { j } : = \theta _ { j } + \alpha \left( y ^ { ( i ) } - h _ { \theta } \left( x ^ { ( i ) } \right) \right) x _ { j } ^ { ( i ) }$$<br>also called <strong>Widrow-Holf</strong> learning rule.<br>2.2. for a training set $( m &gt; 1 )$:</p>
<ul>
<li><strong>batch gradient descent</strong>   <img src="/2019/01/04/Linear-Regression/bgd.PNG" title="BGD"></li>
<li><strong>stochastic gradient descent</strong> ( also <strong>incremental gradient descent</strong> )      <img src="/2019/01/04/Linear-Regression/sgd.PNG" title="SGD">
</li>
</ul>
<h4 id="3-Normal-Equation"><a href="#3-Normal-Equation" class="headerlink" title="3. Normal Equation"></a>3. Normal Equation</h4><p>In the “Normal Equation” method, we will minimize $J$ by explicitly taking its derivatives with respect to the $θ_j$ ’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:<br>$$\theta = \left( X ^ { T } X \right) ^ { - 1 } X ^ { T } y$$</p>
<table>
<thead>
<tr>
<th style="text-align:left">Gradient Descent</th>
<th style="text-align:left">Normal Equation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Need to choose $\alpha$</td>
<td style="text-align:left">No need to choose $\alpha$</td>
</tr>
<tr>
<td style="text-align:left">Needs many iterations</td>
<td style="text-align:left">No need to iterate</td>
</tr>
<tr>
<td style="text-align:left">$O \left( k n ^ { 2 } \right)$</td>
<td style="text-align:left">$O \left( n ^ { 3 } \right)$, need to calculate inverse of $X ^ { T } X$</td>
</tr>
<tr>
<td style="text-align:left">Works well when $n$ is large</td>
<td style="text-align:left">Slow if $n$ is very large</td>
</tr>
</tbody>
</table>
<h4 id="4-Unerfitting-and-Overfitting"><a href="#4-Unerfitting-and-Overfitting" class="headerlink" title="4. Unerfitting and Overfitting"></a>4. Unerfitting and Overfitting</h4><p><strong>Underfitting</strong>, or <em>high bias</em>, is when the form of our hypothesis function $h$ maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features.<br>At the other extreme, <strong>overfitting</strong>, or <em>high variance</em>, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.</p>
<p>This terminology is applied to both linear and logistic regression. There are two main options to address the issue of overfitting:</p>
<p>4.1. Reduce the number of features:</p>
<ul>
<li>Manually select which features to keep.</li>
<li>Use a model selection algorithm (studied later in the course).</li>
</ul>
<p>4.2. Regularization</p>
<ul>
<li>Keep all the features, but reduce the magnitude of parameters $\theta_j$.</li>
<li>Regularization works well when we have a lot of slightly useful features.</li>
</ul>
<h2 id="2-Unsupervised-learning"><a href="#2-Unsupervised-learning" class="headerlink" title="2. Unsupervised learning"></a>2. Unsupervised learning</h2><p>Unsupervised learning allows us to approach problems with little or no idea what our results should look like. </p>
<ul>
<li>Clustering</li>
<li>Non-clustering</li>
</ul>
<h1 id="sklearn"><a href="#sklearn" class="headerlink" title="sklearn"></a>sklearn</h1><h2 id="1-sklearn-linear-model-LinearRegression"><a href="#1-sklearn-linear-model-LinearRegression" class="headerlink" title="1. sklearn.linear_model.LinearRegression"></a>1. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn-linear-model-linearregression" target="_blank" rel="noopener">sklearn.linear_model.LinearRegression</a></h2><h2 id="2-Linear-Regression-Example"><a href="#2-Linear-Regression-Example" class="headerlink" title="2. Linear Regression Example"></a>2. <a href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html" target="_blank" rel="noopener">Linear Regression Example</a></h2><h1 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h1><ol>
<li><a href="https://see.stanford.edu/Course/CS229" target="_blank" rel="noopener">CS229 - Machine Learning</a> </li>
<li>Coursera Machine Learning by Andrew Ng</li>
<li><a href="https://en.wikipedia.org/wiki/Main_Page" target="_blank" rel="noopener">Wikipedia</a></li>
<li><a href="https://scikit-learn.org/stable/index.html" target="_blank" rel="noopener">scikit-learn.org</a></li>
</ol>
<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">This update is <strong><em>simultaneously</em></strong> performed for all values of $j = 0, . . . , n$.</span><a href="#fnref:1" rev="footnote"> ↩</a></li></ol></div></div>
      
    </div>

    
      
      



      
      
    

    
      <footer class="post-footer">
        
          <div class="post-tags">
            
              <a href="/tags/机器学习/">机器学习</a>
            
          </div>
        
        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2019/01/06/Logistic-Regression/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">2. Logistic Regression</span>
        <span class="prev-text nav-mobile">上一篇</span>
      </a>
    
    
      <a class="next" href="/2019/01/03/Datawhale/">
        <span class="next-text nav-default">Datawhale 算法梳理第3期 02号</span>
        <span class="prev-text nav-mobile">下一篇</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>


      </footer>
    

  </article>


          </div>
          
  <div class="comments" id="comments">
    
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNDE0NS8xMDY4Mg==">
        <noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
      </div>  
    
  </div>


        </div>
      </main>

      <footer id="footer" class="footer">

  <div class="social-links">
    
      
        
          <a href="mailto:zhyd007@gmail.com" class="iconfont icon-email" title="email"></a>
        
      
    
      
        
          <a href="https://github.com/Tom007Cheung" class="iconfont icon-github" title="github"></a>
        
      
    
      
        
          <a href="https://weibo.com/5872290257/profile?topnav=1&wvr=6&is_all=1" class="iconfont icon-weibo" title="weibo"></a>
        
      
    
      
        
          <a href="https://www.zhihu.com/people/tom-86-5/activities" class="iconfont icon-zhihu" title="zhihu"></a>
        
      
    
      
        
          <a href="https://www.douban.com/people/156131902/" class="iconfont icon-douban" title="douban"></a>
        
      
    

    
  </div>



<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://hexo.io/">Hexo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">
    
    &copy; 
     
      2018 - 
    
    2019

    <span class="fire">
      <i class="fa fa-fire" aria-hidden="true"></i>
    </span>
    <span class="author">TomCheung</span> 
    <div align="center"><a><img border="0" src="https://cc.amazingcounters.com/counter.php?i=3228910&c=9687043" alt="AmazingCounters.com"></a>
    访客(UV) since 2018/12/31
    </div>
  </span>
</div>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>

    
  
  

  
   <script type="text/javascript">
	(function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
  </script>




    
  



  
  





  
    <script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  

  
    <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  

  
    <script type="text/javascript" src="/lib/pjax/jquery.pjax.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/nprogress/nprogress.min.js"></script>
  


    <script type="text/javascript" src="/js/src/even.js?v=2.10.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  </body>
</html>
